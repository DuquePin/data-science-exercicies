{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1ec7e0-45bc-4a15-b2c4-2affeb50a3cd",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "O algoritmo **Random Forest (RF)** consiste na combinação de vários classificadores fortes para criar um modelo mais robusto e preciso. Basicamente, árvores independentes de decisão são treinadas com o conjunto de dados em amostragens com reposição (**bootstrap sample**) com as linhas (**observações**) e colunas (**features**). A média das previsoes dessas árvores, ou o valor mais frequente se torna a resposta final do modelo.\n",
    "\n",
    "A maior **desvantagem** do algoritmo RF é a o seu alto custo computacional. Vamos observar o porquê disso.\n",
    "\n",
    "## Alto Custo Computacional\n",
    "Para entender o porquê do algoritmo RF ter um alto custo computacional, precisamos observar a sua **complexidade**. A complexidade do algoritmo RF pode ser quebrada em duas etapas: **construção** e **inferência**\n",
    "\n",
    "### Complexidade na Construção\n",
    "Na construção, várias árvores de decisão independentes são criadas, com a complexidade do treinamento de uma única árvore de decisão sendo **proporcional** ao número de amostras e features da base de dados. Então, para cada árvore a sua complexidade é dada por: $O(m \\cdot n \\cdot \\log n)$. Como o algoritmo constroí uma quantidade $T$ de árvores, a complexidade final da etapa de construção é:\n",
    "\n",
    "$$O(T \\cdot m \\cdot n \\cdot \\log n)$$\n",
    "\n",
    "Onde:\n",
    "- $T$ é o número de árvores construídas\n",
    "- $m$ é o número de features da base de dados\n",
    "- $n$ é o número de observações da base de dados\n",
    "- $\\log n$ é o fator de proporcionalidade para árvores balanceadas\n",
    "\n",
    "Ou seja, quanto maior os valores de $T$, $m$ e $n$, mais poder de processamento será necessário para rodar o algoritmo.\n",
    "\n",
    "### Complexidade na Inferência\n",
    "Ao realizar previsões, o modelo passa a amostra por todas as árvores de decisão e agrega as previsões (por média, na caso de regressão, ou por votação, no caso de classificação). O tempo para realizar uma previsão para uma nova amostra em uma única árvore é dado por: $O(\\log n)$. Como árvores balanceadas, cada decisão envolve uma comparação e a árvore é percorrida até a profundidade máxima. Para um número $T$ de árvores, a complexidade final da etapa de inferência é:\n",
    "\n",
    "$$O(T \\cdot \\log n)$$\n",
    "\n",
    "Portanto, a complexidade do algorimto **Random Forest (RF)** é linear ao número de árvores ($T$), número de características ($m$) e número de observações ($n$). A médida que esses valores crescem, maior se torna o custo computacional para rodar o algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492f6ca-4465-4e1b-b1c1-05940c3867b3",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "O algoritmo **Adaptive Boosting (AB)** consiste no treinamento sequencial de classificadores fracos (**weak leaners**), que são apenas um pouco melhores que pequenas árvores de decisão, para a criação de um classificador forte. As previsões de todos eles são combinadas através de voto majoritário ponderado (**weighted majority vote**) ou soma para produzir a resposta final do modelo. Os classificadores fracos são árvores compostas apenas pelo nó raiz e duas folhas, sendo chamadas de **stumps**. A cada etapa de modificação de dados (chamadas **boosting iteration**) são aplicados **pesos** para cada observação das amostras de treinamento com reposições (**bootstrap sample**). Inicialmente, todos os **pesos** são iguais a: $w_i = 1 / N$. Onde $N$ é o número de observações (linhas) da base de dados. Para cada iteração após a primeira, os pesos das observações são individualmente modificados com base na performance do melhor stump da iteração anterior, dado por:\n",
    "\n",
    "$$w_i = w_i \\cdot e^{(\\pm \\rho)}$$\n",
    "\n",
    "Onde $\\rho$ é a **performance** do stump, calculado pela expressão:\n",
    "\n",
    "$$\\rho = \\frac{1}{2} \\cdot \\ln (\\frac{QtdAcertos}{QtdErros})$$\n",
    "\n",
    "O sinal de $\\rho$ é positivo quando a previsão está incorreta e negativo quando está correta para cada observação individual. Dessa forma, o algoritmo aumenta os pesos das linhas (observações) com previsões incorretas e diminuí os pesos das demais linhas. Isso faz com que o modelo foque nos erros, permitindo refinar sua acurácia a cada iteração.\n",
    "\n",
    "Ao final do processo de treino, os pesos são **normalizados** para que sua soma total seja igual a 1 e o processo é repetido na próxima iteração com os novos pesos. Os valores das **classes** são somados e a classe com a maior soma é a resposta final do modelo.\n",
    "\n",
    "A maior **vantagem** do algoritmo AB quando comparado ao RF é o seu baixo custo computacional. Vamos observar o porquê disso.\n",
    "\n",
    "## Baixo Custo Computacional\n",
    "Semelhante ao algoritmo **Random Forest**, a complexidade do algoritmo AB pode ser quebrada em duas etapas: **construção** e **inferência**.\n",
    "\n",
    "### Complexidade na Construção\n",
    "O **AdaBoost** utiliza múltiplos classificadores fracos (geralmente árvores de decisão de profundidade 1, chamadas de stumps) e ajusta seus pesos para formar um classificador final forte. O treinamento do modelo consiste em treinar stumps de forma **sequencial**, ajustando o peso das amostras a cada rodada, de modo que as amostras classificadas incorretamente recebam maior peso nas rodadas subsequentes.\n",
    "\n",
    "Para cada stump, a complexidade de treinamento é a mesma que a de treinar esse classificador em particular. Portanto, para um stump padrão, a complexidade será proporcional ao número de observações e features. Sendo a complexidade dada por $O (m \\cdot n)$. Como o modelo treina um número $T$ de stumps, a complexidade final da etapa de construção é:\n",
    "\n",
    "$$O(T \\cdot m \\cdot n)$$\n",
    "\n",
    "Onde:\n",
    "- $T$ é o número de árvores (stumps)\n",
    "- $m$ é o número de features\n",
    "- $n$ é o número de observações\n",
    "  \n",
    "Além disso, o algoritmo envolve o ajuste dos pesos das amostras após cada iteração, o que também exige $O(n)$ operações por iteração, mas isso não altera a ordem de complexidade dominante.\n",
    "\n",
    "### Complexidade na Inferência\n",
    "Durante a previsão, o AdaBoost faz com que cada stump contribuia com uma previsão ponderada para a decisão final. Para uma nova amostra, a previsão é obtida somando as previsões ponderadas de todos os $T$ classificadores. A complexidade desse processo é $O(m)$, como ele deve ser repetido para cada $T$, a complexidade final da etapa de inferência é:\n",
    "\n",
    "$$O(T \\cdot m)$$\n",
    "\n",
    "Portanto, assim como o algoritmo de **Random Forest**, a complexidade do **AdaBoost** é linear ao número de árvores treinadas, observações e features. Contudo, o principal motivo do baixo custo computacional do algoritmo, quando comparado ao RF, é o fato dele criar árvores muito mais simples, o que simplifica muitas operações e agiliza o processo de construção e inferência do algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a499c71-d631-4aed-95a8-595570e6ae96",
   "metadata": {},
   "source": [
    "# Comparação RF vs. AB\n",
    "\n",
    "## Complexidade\n",
    "Algoritmo RF tem complexidade linear dividida em duas etaps:\n",
    "- Complexidade na construção (treino) = $O(T \\cdot m \\cdot n \\cdot \\log n)$\n",
    "- Complexidade na inferência (previsão) = $O(T \\cdot \\log n)$\n",
    "\n",
    "Algoritmo AB tem complexidade linear dividida em duas etapas:\n",
    "- Complexidade na construção (treino) = $O(T \\cdot m \\cdot n)$\n",
    "- Complexidade na inferência (previsão) = $O(T \\cdot m)$\n",
    "\n",
    "## Modelos\n",
    "Algoritmo RF usa árvores de decisão **independentes** e **complexas** de profundidades variáveis para construir seus modelos.\n",
    "\n",
    "Algoritmo AB usa árvores de decisão **dependentes** e **simples** de profundidades constantes igual a **1** para construir seus modelos.\n",
    "\n",
    "## Inferência\n",
    "Algoritmo RF usa média ou votação majoritária para criar a resposta final.\n",
    "\n",
    "Algoritmo AB usa média ponderada ou votação majoritária ponderada para criar a resposta final.\n",
    "\n",
    "## Sensibilidade\n",
    "Algoritmo RF é mais robusto e menos propenso a **overfitting** por conta das árvores complexas que utiliza para o treinamento dos modelos.\n",
    "\n",
    "Algoritmo AB é mais sensível a ruídos e pode sofrer de **overfitting** mais facilmente, especialmente se houver outliers, por conta do ajuste de pesos de amostras mal classificadas, outliers podem receber pesos muito altos.\n",
    "\n",
    "## Velocidade\n",
    "Algoritmo RF leva mais tempo para ser treinado e consome mais recursos computacionais devido a complexidade dos seus modelos.\n",
    "\n",
    "Algoritmo AB leva menos tempo para ser treinado e consome menos recursos computacionais devido a simplicidade dos seus modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c5626-387e-4544-91d7-31f8cfc0c254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
